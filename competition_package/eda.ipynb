{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages required for plotly rendering in notebooks\n",
    "!pip install -q --upgrade jupyter \"ipywidgets>=7.6.0\" \"nbformat>=4.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c3523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "p = Path(\"datasets\") / \"train.parquet\"\n",
    "df = pd.read_parquet(p)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Select the first sequence from the dataframe\n",
    "seq_id_to_plot = 0\n",
    "df_seq = df[df['seq_ix'] == seq_id_to_plot]\n",
    "\n",
    "# Get the list of feature columns (f_0 to f_31)\n",
    "feature_cols = [f'{i}' for i in range(32)]\n",
    "\n",
    "# Generate a separate plot for each feature\n",
    "for feature in feature_cols:\n",
    "    fig = px.line(df_seq, x='step_in_seq', y=feature, title=f'{feature} for Sequence {seq_id_to_plot}')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6be2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492826e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the first sequence from the dataframe\n",
    "seq_id_to_plot = 1\n",
    "feature_cols = [f'{i}' for i in range(32)]\n",
    "df_seq = df[df['seq_ix'] == seq_id_to_plot]\n",
    "\n",
    "# Compute the correlation matrix for all features in the sequence\n",
    "corr_matrix = df_seq[feature_cols].corr()\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', xticklabels=feature_cols, yticklabels=feature_cols)\n",
    "plt.title(f'Correlation Matrix for Features of Sequence {seq_id_to_plot}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f974da",
   "metadata": {},
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Select a sequence to plot\n",
    "seq_id_to_plot = 0\n",
    "feature_cols = [f'{i}' for i in range(32)]\n",
    "df_seq = df[df['seq_ix'] == seq_id_to_plot]\n",
    "\n",
    "window = 10  # window size for SMA/EMA\n",
    "alpha = 0.2  # smoothing factor for EMA\n",
    "\n",
    "# Create subplot grid: 8 rows x 4 columns\n",
    "fig = make_subplots(rows=8, cols=4, subplot_titles=[f'Feature {f}' for f in feature_cols], shared_xaxes=True)\n",
    "\n",
    "for idx, feature in enumerate(feature_cols):\n",
    "    values = df_seq[feature].values\n",
    "    sma = pd.Series(values).rolling(window=window, min_periods=1).mean().values\n",
    "    ema = pd.Series(values).ewm(alpha=alpha, adjust=False).mean().values\n",
    "    row = idx // 4 + 1\n",
    "    col = idx % 4 + 1\n",
    "    fig.add_trace(go.Scatter(x=df_seq['step_in_seq'], y=values, mode='lines', name='Value', line=dict(color='gray', width=1), opacity=0.6, showlegend=(idx==0)), row=row, col=col)\n",
    "    fig.add_trace(go.Scatter(x=df_seq['step_in_seq'], y=sma, mode='lines', name='SMA', line=dict(color='blue', width=2), showlegend=(idx==0)), row=row, col=col)\n",
    "    fig.add_trace(go.Scatter(x=df_seq['step_in_seq'], y=ema, mode='lines', name='EMA', line=dict(color='red', width=2), showlegend=(idx==0)), row=row, col=col)\n",
    "\n",
    "fig.update_layout(height=2000, width=1600, title_text=f'Sequence {seq_id_to_plot}: Value, SMA, and EMA for All Features', legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1))\n",
    "fig.update_xaxes(title_text='step_in_seq', row=8)\n",
    "fig.update_yaxes(title_text='Value', col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d3687",
   "metadata": {},
   "source": [
    "fft and x(t) vs x(t+1)-x(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go  # Added Plotly\n",
    "from plotly.offline import plot      # Added Plotly offline plot\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from plotting\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Loading data...\")\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = df\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Using a dummy dataset for demonstration.\")\n",
    "    # Create a dummy dataset if loading fails\n",
    "    num_sequences = 10\n",
    "    num_steps = 1000\n",
    "    num_features = 5\n",
    "    data = []\n",
    "    for seq_id in range(num_sequences):\n",
    "        for step in range(num_steps):\n",
    "            row = {'seq_ix': seq_id, 'step_in_seq': step, 'need_prediction': step >= 100}\n",
    "            row.update({f'feature_{i}': np.sin(step / (50 + i*10)) + np.random.randn()*0.1 for i in range(num_features)})\n",
    "            data.append(row)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in (\"seq_ix\", \"step_in_seq\", \"need_prediction\")]\n",
    "num_features = len(feature_cols)\n",
    "\n",
    "# --- Isolate one sequence for analysis ---\n",
    "sample_seq_ix = df['seq_ix'].unique()[3]\n",
    "one_seq_df = df[df['seq_ix'] == sample_seq_ix].sort_values('step_in_seq')\n",
    "states = one_seq_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "print(f\"Analyzing sample sequence {sample_seq_ix} with shape {states.shape}\")\n",
    "\n",
    "# === 1. Compute FFT for each column ===\n",
    "print(\"\\n--- 1. FFT Analysis ---\")\n",
    "N_T = states.shape[0] # Number of timesteps (1000)\n",
    "frequencies = np.fft.fftfreq(N_T, d=1) # Assumes 1 unit of time per step\n",
    "num_features_to_plot = min(5, num_features)\n",
    "\n",
    "# --- Matplotlib FFT Plot ---\n",
    "plt.figure(figsize=(12, 7))\n",
    "for i in range(num_features_to_plot):\n",
    "    fft_values = np.fft.fft(states[:, i])\n",
    "    fft_power = np.abs(fft_values)\n",
    "    plt.plot(frequencies[:N_T // 2], fft_power[:N_T // 2], label=f'Feature {i}', alpha=0.7)\n",
    "\n",
    "plt.title('FFT Power Spectrum (First 5 Features) - Matplotlib', fontsize=16)\n",
    "plt.xlabel('Frequency', fontsize=12)\n",
    "plt.ylabel('Magnitude', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('fft_spectrum.png')\n",
    "print(\"Saved Matplotlib FFT plot to fft_spectrum.png\")\n",
    "\n",
    "# --- Plotly FFT Plot (New) ---\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(num_features_to_plot):\n",
    "    fft_values = np.fft.fft(states[:, i])\n",
    "    fft_power = np.abs(fft_values)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=frequencies[:N_T // 2], \n",
    "        y=fft_power[:N_T // 2], \n",
    "        mode='lines',\n",
    "        name=f'Feature {i}'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Interactive FFT Power Spectrum (First 5 Features) - Plotly',\n",
    "    xaxis_title='Frequency',\n",
    "    yaxis_title='Magnitude',\n",
    "    hovermode=\"x unified\",\n",
    "    legend_title=\"Features\"\n",
    ")\n",
    "plot(fig, filename='fft_spectrum.html', auto_open=False)\n",
    "print(\"Saved Plotly FFT plot to fft_spectrum.html\")\n",
    "\n",
    "\n",
    "# === 2. Calculate correlations of x(t) with x(t+1) - x(t) ===\n",
    "print(\"\\n--- 2. x(t) vs. Delta Correlation Analysis ---\")\n",
    "\n",
    "x_t = states[:-1]\n",
    "x_t_plus_1 = states[1:]\n",
    "delta = x_t_plus_1\n",
    "\n",
    "correlations = []\n",
    "for i in range(num_features):\n",
    "    corr, _ = pearsonr(x_t[:, i], delta[:, i])\n",
    "    correlations.append(corr)\n",
    "\n",
    "correlations = np.array(correlations)\n",
    "\n",
    "print(f\"Mean correlation: {np.nanmean(correlations):.4f}\")\n",
    "print(f\"Min correlation:  {np.nanmin(correlations):.4f}\")\n",
    "print(f\"Max correlation:  {np.nanmax(correlations):.4f}\")\n",
    "\n",
    "mean_r_squared = np.nanmean(correlations**2)\n",
    "print(f\"\\nMean R-squared (as a linear predictor): {mean_r_squared:.6f}\")\n",
    "\n",
    "\n",
    "# === 3. Plot x(t) with x(t+1) - x(t) ===\n",
    "print(\"\\n--- 3. Scatter Plot Analysis ---\")\n",
    "plt.clf() # Clear the previous FFT plot\n",
    "\n",
    "feature_to_plot = 0\n",
    "x_data = x_t[:, feature_to_plot]\n",
    "y_data = delta[:, feature_to_plot]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_data, y_data, alpha=0.1, s=10)\n",
    "plt.title(f'State vs. Next Change (Feature {feature_to_plot}, Seq {sample_seq_ix})', fontsize=16)\n",
    "plt.xlabel(f'x(t) [State of Feature {feature_to_plot}]', fontsize=12)\n",
    "plt.ylabel(f'x(t+100) - x(t) [Next Delta of Feature {feature_to_plot}]', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=1)\n",
    "plt.savefig('xt_vs_delta_scatter.png')\n",
    "print(\"Saved scatter plot to xt_vs_delta_scatter.png\")\n",
    "print(\"\\nAnalysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1=0\n",
    "sum2=0\n",
    "n_seq = df['seq_ix'].nunique()\n",
    "for i in range(len(df)):\n",
    "    if df['need_prediction'][i]==1:\n",
    "        sum1+=1\n",
    "    else:\n",
    "        sum2+=1\n",
    "print(f'need pred: {sum1} \\n do not need pred: {sum2}.')\n",
    "print(f'avg need pred: {sum1/n_seq} \\n do not need pred: {sum2/n_seq}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c2393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f84d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters: choose sequence and feature to analyze\n",
    "seq_id_to_plot = 515  # Change as needed\n",
    "feature_to_plot = '14'  # Change as needed (e.g., '0', '1', ..., '31')\n",
    "max_lag = 100  # Number of lags to compute\n",
    "\n",
    "\n",
    "# Extract the sequence and feature values\n",
    "df_seq = df[df['seq_ix'] == seq_id_to_plot].sort_values('step_in_seq')\n",
    "values = df_seq[feature_to_plot].values\n",
    "\n",
    "# Compute correlations for each lag\n",
    "correlations = []\n",
    "lags = np.arange(1, max_lag + 1)\n",
    "for lag in lags:\n",
    "    if len(values) > lag:\n",
    "        # Calculate correlation between x(t) and x(t-lag)\n",
    "        corr = np.corrcoef(values[lag:], values[:-lag])[0, 1]\n",
    "        correlations.append(corr)\n",
    "    else:\n",
    "        correlations.append(np.nan)\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=lags, y=correlations, mode='lines+markers', name=f'Feature {feature_to_plot}'))\n",
    "fig.update_layout(title=f'Autocorrelation of Feature {feature_to_plot} in Sequence {seq_id_to_plot}',\n",
    "                  xaxis_title='Lag', yaxis_title='Correlation',\n",
    "                  height=400, width=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29168ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8069c868",
   "metadata": {},
   "source": [
    "### 1. Feature Distribution and Outlier Analysis\n",
    "\n",
    "This analysis visualizes the distribution of each of the 32 features across the entire dataset. \n",
    "\n",
    "- **Histograms**: Show the frequency of values in different bins, helping to identify the shape of the distribution (e.g., Gaussian, skewed, bimodal).\n",
    "- **Box Plots**: Display the five-number summary of a set of data: minimum, first quartile, median, third quartile, and maximum. They are useful for quickly identifying the data's spread and detecting outliers.\n",
    "\n",
    "The following cell will generate these plots for all features. You can hover over the interactive plots to see detailed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645edd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "feature_cols = [f'{i}' for i in range(32)]\n",
    "\n",
    "# Create subplots: 1 row for histograms, 1 for box plots, across all 32 features\n",
    "# To make it manageable, we'll create 4 rows of plots, 8 features per row.\n",
    "fig = make_subplots(\n",
    "    rows=8, cols=4, \n",
    "    subplot_titles=[f'Feature {f}' for f in feature_cols],\n",
    "    specs=[[{'type': 'histogram'}, {'type': 'histogram'}, {'type': 'histogram'}, {'type': 'histogram'}] for _ in range(8)]\n",
    ")\n",
    "\n",
    "# Add histograms\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    row = i // 4 + 1\n",
    "    col = i % 4 + 1\n",
    "    fig.add_trace(go.Histogram(x=df[feature], name=f'Hist {feature}', nbinsx=100), row=row, col=col)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=2000, \n",
    "    width=1600, \n",
    "    title_text='Feature Distributions (Histograms)',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Create box plots in a separate figure for clarity\n",
    "fig_box = make_subplots(\n",
    "    rows=4, cols=8, \n",
    "    subplot_titles=[f'Feature {f}' for f in feature_cols]\n",
    ")\n",
    "\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    row = i // 8 + 1\n",
    "    col = i % 8 + 1\n",
    "    fig_box.add_trace(go.Box(y=df[feature], name=f'Box {feature}'), row=row, col=col)\n",
    "\n",
    "fig_box.update_layout(\n",
    "    height=1000, \n",
    "    width=1600, \n",
    "    title_text='Feature Spread and Outliers (Box Plots)',\n",
    "    showlegend=False\n",
    ")\n",
    "fig_box.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf0a33",
   "metadata": {},
   "source": [
    "### 2. Cross-Correlation Analysis\n",
    "\n",
    "Cross-correlation measures the similarity between two time series as a function of the displacement of one relative to the other. It helps to identify if one feature is a leading or lagging indicator of another.\n",
    "\n",
    "The plot below shows the correlation between `feature_a` and `feature_b` at different time lags.\n",
    "- A peak at a **positive lag** means `feature_a` leads `feature_b`.\n",
    "- A peak at a **negative lag** means `feature_b` leads `feature_a`.\n",
    "- A peak at **lag 0** indicates a simultaneous correlation.\n",
    "\n",
    "You can modify the `feature_a`, `feature_b`, `seq_id_to_analyze`, and `max_lag` parameters in the next cell to explore different relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde74268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --- Parameters ---\n",
    "feature_a = '0'\n",
    "feature_b = '1'\n",
    "seq_id_to_analyze = 0\n",
    "max_lag = 50\n",
    "\n",
    "# --- Data Extraction ---\n",
    "df_seq = df[df['seq_ix'] == seq_id_to_analyze].sort_values('step_in_seq')\n",
    "values_a = df_seq[feature_a].values\n",
    "values_b = df_seq[feature_b].values\n",
    "\n",
    "# --- Cross-Correlation Calculation ---\n",
    "# Standardize the series (z-score normalization)\n",
    "values_a = (values_a - np.mean(values_a)) / (np.std(values_a) * len(values_a))\n",
    "values_b = (values_b - np.mean(values_b)) / np.std(values_b)\n",
    "\n",
    "lags = np.arange(-max_lag, max_lag + 1)\n",
    "corrs = [np.correlate(values_a, np.roll(values_b, lag))[0] for lag in lags]\n",
    "\n",
    "# --- Plotting ---\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=lags, \n",
    "    y=corrs, \n",
    "    mode='lines+markers',\n",
    "    name=f'Cross-corr F{feature_a} vs F{feature_b}'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=f'Cross-Correlation between Feature {feature_a} and Feature {feature_b} (Seq {seq_id_to_analyze})',\n",
    "    xaxis_title='Lag (Displacement of Feature B relative to A)',\n",
    "    yaxis_title='Correlation',\n",
    "    height=400,\n",
    "    width=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927f4a0",
   "metadata": {},
   "source": [
    "### 3. Volatility and Change Analysis\n",
    "\n",
    "This section examines the stability of the features by looking at two key metrics:\n",
    "\n",
    "- **First-Order Difference (Delta)**: Calculated as `x(t) - x(t-1)`, this shows the step-by-step change in a feature's value. A histogram of these deltas can reveal if the changes are typically small and centered around zero, or if there are frequent large jumps.\n",
    "- **Rolling Standard Deviation**: This measures the volatility of a feature over a sliding window of time. Peaks in the rolling standard deviation indicate periods of high fluctuation.\n",
    "\n",
    "You can adjust the `feature_to_analyze`, `seq_id_to_analyze`, and `rolling_window_size` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adfe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# --- Parameters ---\n",
    "feature_to_analyze = '0'\n",
    "seq_id_to_analyze = 0\n",
    "rolling_window_size = 50\n",
    "\n",
    "# --- Data Extraction ---\n",
    "df_seq = df[df['seq_ix'] == seq_id_to_analyze].sort_values('step_in_seq').copy()\n",
    "values = df_seq[feature_to_analyze]\n",
    "\n",
    "# --- Calculations ---\n",
    "# First-order difference\n",
    "df_seq['delta'] = values.diff()\n",
    "# Rolling standard deviation\n",
    "df_seq['volatility'] = values.rolling(window=rolling_window_size).std()\n",
    "\n",
    "# --- Plotting ---\n",
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True, subplot_titles=(\n",
    "    f'Feature {feature_to_analyze} Values',\n",
    "    'First-Order Difference (Delta)',\n",
    "    f'Rolling {rolling_window_size}-Step Volatility'\n",
    "))\n",
    "\n",
    "# Plot original feature values\n",
    "fig.add_trace(go.Scatter(x=df_seq['step_in_seq'], y=df_seq[feature_to_analyze], mode='lines', name='Value'), row=1, col=1)\n",
    "# Plot deltas\n",
    "fig.add_trace(go.Scatter(x=df_seq['step_in_seq'], y=df_seq['delta'], mode='lines', name='Delta', line=dict(color='orange')), row=2, col=1)\n",
    "# Plot volatility\n",
    "fig.add_trace(go.Scatter(x=df_seq['step_in_seq'], y=df_seq['volatility'], mode='lines', name='Volatility', line=dict(color='red')), row=3, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800, \n",
    "    width=1200, \n",
    "    title_text=f'Volatility Analysis for Feature {feature_to_analyze} in Sequence {seq_id_to_analyze}',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Histogram of Deltas\n",
    "fig_hist = go.Figure()\n",
    "fig_hist.add_trace(go.Histogram(x=df_seq['delta'].dropna(), nbinsx=100))\n",
    "fig_hist.update_layout(\n",
    "    title=f'Distribution of Deltas for Feature {feature_to_analyze} (Seq {seq_id_to_analyze})',\n",
    "    xaxis_title='Delta (x(t) - x(t-1))',\n",
    "    yaxis_title='Frequency',\n",
    "    height=400,\n",
    "    width=800\n",
    ")\n",
    "fig_hist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0cd2f2",
   "metadata": {},
   "source": [
    "### 4. Sequence-Level Clustering\n",
    "\n",
    "This analysis aims to discover if there are distinct \"types\" of sequences in the dataset. The process is as follows:\n",
    "\n",
    "1.  **Feature Engineering**: For each sequence, a set of summary statistics (mean, std, min, max, etc.) is calculated for each of the 32 features. This creates a single \"summary\" vector that represents the overall behavior of each sequence.\n",
    "2.  **Dimensionality Reduction**: The high-dimensional summary vectors are scaled and then reduced to 2 dimensions using Principal Component Analysis (PCA). This allows us to visualize the sequences in a 2D scatter plot.\n",
    "3.  **Clustering**: The K-Means algorithm is applied to the PCA-transformed data to group the sequences into a predefined number of clusters.\n",
    "\n",
    "The resulting plot shows each sequence as a point, colored by its assigned cluster. This can reveal natural groupings and help in understanding the different dynamic regimes present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "\n",
    "# --- Parameters ---\n",
    "n_clusters = 5  # You can experiment with this value\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "feature_cols = [f'{i}' for i in range(32)]\n",
    "# Calculate aggregate statistics for each sequence\n",
    "agg_funcs = ['mean', 'std', 'min', 'max', 'median']\n",
    "seq_summary = df.groupby('seq_ix')[feature_cols].agg(agg_funcs)\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "seq_summary.columns = ['_'.join(col).strip() for col in seq_summary.columns.values]\n",
    "seq_summary.dropna(inplace=True) # Drop sequences that might have NaNs (e.g., if std is zero)\n",
    "\n",
    "# --- Scaling and PCA ---\n",
    "scaler = StandardScaler()\n",
    "summary_scaled = scaler.fit_transform(seq_summary)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "summary_pca = pca.fit_transform(summary_scaled)\n",
    "\n",
    "# --- K-Means Clustering ---\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(summary_pca)\n",
    "\n",
    "# --- Visualization ---\n",
    "pca_df = pd.DataFrame(summary_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['cluster'] = clusters\n",
    "pca_df['seq_ix'] = seq_summary.index\n",
    "\n",
    "fig = px.scatter(\n",
    "    pca_df, \n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    color='cluster',\n",
    "    hover_data=['seq_ix'],\n",
    "    title=f'Sequence-Level Clustering (K={n_clusters})',\n",
    "    labels={'color': 'Cluster ID'}\n",
    ")\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1000\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b1cde",
   "metadata": {},
   "source": [
    "### 5. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a technique used to emphasize variation and bring out strong patterns in a dataset. It's often used for dimensionality reduction.\n",
    "\n",
    "1.  **Explained Variance**: This plot shows the percentage of the total variance in the dataset that is captured by each principal component. It helps to determine how many components are needed to represent a significant portion of the data's structure.\n",
    "2.  **PCA Scatter Plot**: This plot visualizes the entire dataset in a 2D space defined by the first two principal components. We can color the points by a specific feature or by the `need_prediction` flag to see if these components separate the data in a meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# --- Data Preparation ---\n",
    "feature_cols = [f'{i}' for i in range(32)]\n",
    "# We'll use a sample of the data to keep the plot from getting too crowded\n",
    "df_sample = df.sample(n=min(50000, len(df)), random_state=42)\n",
    "\n",
    "X = df_sample[feature_cols]\n",
    "y = df_sample['need_prediction']\n",
    "\n",
    "# --- Scaling and PCA ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# --- Explained Variance Plot ---\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "fig_var = go.Figure()\n",
    "fig_var.add_trace(go.Bar(x=np.arange(1, len(explained_variance) + 1), y=explained_variance, name='Individual Explained Variance'))\n",
    "fig_var.add_trace(go.Scatter(x=np.arange(1, len(cumulative_variance) + 1), y=cumulative_variance, name='Cumulative Explained Variance'))\n",
    "\n",
    "fig_var.update_layout(\n",
    "    title='Explained Variance by Principal Components',\n",
    "    xaxis_title='Principal Component',\n",
    "    yaxis_title='Explained Variance Ratio',\n",
    "    height=500,\n",
    "    width=1000\n",
    ")\n",
    "fig_var.show()\n",
    "\n",
    "# --- PCA Scatter Plot ---\n",
    "pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "pca_df['need_prediction'] = y.values\n",
    "\n",
    "fig_scatter = px.scatter(\n",
    "    pca_df,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='need_prediction',\n",
    "    title='PCA of Features (colored by need_prediction)',\n",
    "    opacity=0.5\n",
    ")\n",
    "fig_scatter.update_layout(\n",
    "    height=600,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig_scatter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56ad11",
   "metadata": {},
   "source": [
    "### 6. Comparing Data Segments (`need_prediction` == True vs. False)\n",
    "\n",
    "This analysis investigates whether the distribution of the features changes between the time steps where we have historical data (`need_prediction = False`) and the time steps where we need to make predictions (`need_prediction = True`).\n",
    "\n",
    "A significant difference in the distributions could imply that the underlying process is non-stationary, and a model trained on the first part of the sequence might not perform well on the second part.\n",
    "\n",
    "**Violin plots** are used for this comparison. They are similar to box plots but also show the probability density of the data at different values. This provides a more detailed view of the distribution's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82459b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "feature_cols = [f'{i}' for i in range(32)]\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=8, cols=4, \n",
    "    subplot_titles=[f'Feature {f}' for f in feature_cols]\n",
    ")\n",
    "\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    row = i // 4 + 1\n",
    "    col = i % 4 + 1\n",
    "    \n",
    "    # Add violin plot for need_prediction = False\n",
    "    fig.add_trace(go.Violin(\n",
    "        y=df[df['need_prediction'] == False][feature],\n",
    "        name='False',\n",
    "        legendgroup='group1',\n",
    "        scalegroup='group1',\n",
    "        side='negative',\n",
    "        box_visible=True,\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        showlegend=(i==0)\n",
    "    ), row=row, col=col)\n",
    "    \n",
    "    # Add violin plot for need_prediction = True\n",
    "    fig.add_trace(go.Violin(\n",
    "        y=df[df['need_prediction'] == True][feature],\n",
    "        name='True',\n",
    "        legendgroup='group2',\n",
    "        scalegroup='group2',\n",
    "        side='positive',\n",
    "        box_visible=True,\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        showlegend=(i==0)\n",
    "    ), row=row, col=col)\n",
    "\n",
    "fig.update_traces(meanline_visible=True)\n",
    "fig.update_layout(\n",
    "    height=2000, \n",
    "    width=1600, \n",
    "    title_text='Feature Distributions by need_prediction Flag',\n",
    "    violingap=0, \n",
    "    violingroupgap=0, \n",
    "    violinmode='overlay',\n",
    "    legend_title=\"need_prediction\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206bc594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of unique sequences\n",
    "total_sequences = df['seq_ix'].nunique()\n",
    "print(f\"Total number of sequences: {total_sequences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78905e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
